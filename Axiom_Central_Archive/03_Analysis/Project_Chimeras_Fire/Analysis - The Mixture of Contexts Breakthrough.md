# Mixture of Contexts for Long Video Generation

â€œMinute-long context memory with short-video costâ€

[Shengqu Cai](https://primecai.github.io/)1,*Â [Ceyuan Yang](https://ceyuan.me/)2,â€ Â [Lvmin Zhang](https://lllyasviel.github.io/lvmin_zhang/)1Â [Yuwei Guo](https://guoyww.github.io/)4Â [Junfei Xiao](https://lambert-x.github.io/)3Â [Ziyan Yang](https://ziyanyang.github.io/)2Â [Yinghao Xu](https://justimyhxu.github.io/)1Â [Zhenheng Yang](https://zhenheny.github.io/)5Â [Alan Yuille](https://www.cs.jhu.edu/~ayuille/)3Â [Leonidas Guibas](https://profiles.stanford.edu/leonidas-guibas)1Â [Maneesh Agrawala](https://graphics.stanford.edu/~maneesh/)1Â [Lu Jiang](http://www.lujiang.info/)2Â [Gordon Wetzstein](https://stanford.edu/~gordonwz/)1

1Stanford UniversityÂ 2ByteDance SeedÂ 3Johns Hopkins UniversityÂ 4CUHKÂ 5ByteDance

*Work done at ByteDance Seed Â Â â€ Corresponding Author

[ğŸ“„Research Paper](http://arxiv.org/abs/2508.21058)

LearnableÂ SparseÂ AttentionÂ Routing

A non-parametric yet learnable router selects informative history chunks to calculate attention. The routing is implicitly differentiable and optimized end-to-end from large-scale long video data.

Multi-shotÂ LongÂ VideoÂ Generation

For 8-shot, minute-long videos (~180k tokens), MoC prunesÂ â‰ˆÂ 85 % of token pairs and cuts FLOPs by 7Ã— while delivering minute-long context coherence.

_Some prompts are from Twitter artworks._

Single-shotÂ ShortÂ VideoÂ Generation

On 8-second, 320Ã—192 clips (~6.5k tokens), MoC routing prunesÂ â‰ˆÂ 83 % of token pairs, while maintaining or even improving video quality.

BibTeX

Copy

@inproceedings{cai2025moc,
        title={Mixture of Contexts for Long Video Generation},
        author={Cai, Shengqu and Yang, Ceyuan and Zhang, Lvmin and Guo, Yuwei and Xiao, Junfei and Yang, Ziyan and Xu, Yinghao and Yang, Zhenheng and Yuille, Alan and Guibas, Leonidas and Agrawala, Maneesh and Jiang, Lu and Wetzstein, Gordon},
        year={2025},
        booktitle={arXiv},
      }